# -*- coding: utf-8 -*-
"""Atlas_git.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B0m6MMJ3f6D9P2s6Xa-PQk802LpuT3a2

Общий план задания
1. Скачать [http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/) файлы соответствующие хромосомам 1-22.
2. Перевести с помощью plink https://www.cog-genomics.org/plink/1.9/ из vcf в bed\bim\fam
3. Оставить в данных только SNP с помощью plink.
4. Слить полученные файлы в один набор bed\bim\fam
5. Изменить fam файл, чтобы в нем была информация о популяциях. Соответствие образец-популяция можно найти на сайт 1000 геномов.
6. Сделать новый набор данных, чтобы в нем были только европейские популяции.
7. http://software.genetics.ucla.edu/admixture/ - с помощью этой программы сделать анализ по 5 компонентам (базовый запуск можно найти в документации).

Как результат ожидается следующее:

а. Текстовый файл, какие команды использовались для каждого шага (чтобы можно было воспроизвести).

б.Q файл по итогам 7 шага

в. Графики разложения отельных индивидов на компоненты

![Общий план решения:](https://pics.me.me/thumb_step-to-step-guide-step-1-rotate-the-sides-52304386.png)

#**1. Скачать http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ файлы соответствующие хромосомам 1-22**.

![waiting](https://i.pinimg.com/736x/3e/b8/8a/3eb88a6328279957b75f3c25434510b1.jpg)

Быстрый вариант:

!echo http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chr{1..22}.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz.tbi | xargs -P22 -n1 wget -nv
"""

for i in range(1, 23):
    !wget http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chr{str(i)}.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz

"""# **2. Перевести с помощью plink https://www.cog-genomics.org/plink/1.9/ из vcf в bed\bim\fam**
# **3. Оставить в данных только SNP с помощью plink.**
<img src="https://monosnap.com/image/9q9JeybjDfnEec70wR2NVQnu4vFBIb" width="340" height="270">
"""

# Устанавливаем PLINK
!wget http://s3.amazonaws.com/plink2-assets/alpha2/plink2_linux_x86_64.zip
!unzip plink_linux_x86_64_20200121.zip -d plink/

#Устанавливаем и импортируем необходимые библиотеки для работы с таблицей
#!pip install pandas
#!pip install xlrd
import pandas as pd

#clear_output()
for chr_num in range(12, 13):
    #if chr_num > 1: #1
    #Распаковываем хромосому в bed-bim-fam файлы
    !plink/plink --vcf ALL.chr{str(chr_num)}.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz --make-bed --out ALL.chr{str(chr_num)}
    #Оставляем только SNP
    !plink/plink --bfile ALL.chr{str(chr_num)} --snps-only --make-bed --out ALL.chr{str(chr_num)}

    #удаляем длинные SNP's из bim файла
    df_bim = pd.read_csv(f'ALL.chr{str(chr_num)}.bim', names = ['strings'], header = None, index_col = False)
    very_long_string =[]
    cnt_str = 0
    for cnt, string in enumerate(df_bim.strings):
        lstrings = string.split('\t')
        for lstring in lstrings:
            if ';' in lstring:
                very_long_string.append(lstring)
                #print (cnt, lstrings, lstring, len(lstring))
                cnt_str +=1
    print (' ')
    print (f'Найдено длинных SNPs в {str(chr_num)} хромосоме: {cnt_str}')
    print ('удаляем... ')
    with open (f'id_length{str(chr_num)}.txt', 'a') as idl:
        for string in very_long_string:        
            idl.write(string + '\n')
    !plink/plink --bfile ALL.chr{str(chr_num)} --exclude id_length{str(chr_num)}.txt --make-bed --out ALL.chr{str(chr_num)}
    
    #Удаляем дубликаты одинаковых SNPs
    !cut -f2 ALL.chr{str(chr_num)}.bim | sort | uniq -d > dup{str(chr_num)}.snps
    print (' ')
    print (f'найдено дупликатов в {str(chr_num)} хромосоме: ')
    !wc -l dup{str(chr_num)}.snps
    !echo ' - удаляю...'
    print (' ')
    !plink/plink --bfile ALL.chr{str(chr_num)} --exclude dup{str(chr_num)}.snps --make-bed --out ALL.chr{str(chr_num)}

"""# **4.Слить полученные файлы в один набор bed\bim\fam**
<img src="https://monosnap.com/image/avZ7PrPOHJw3zmx5sDknRs6aUuz2mC" width="250" height="200"/>
"""

#Создаем список всех файлов для объединения в один набор
!rm merge_list.txt
for i in range(1, 23):
    with open('merge_list.txt', 'a') as mlist:
        mlist.write(f'ALL.chr{str(i)}.bed ALL.chr{str(i)}.bim ALL.chr{str(i)}.fam \n')

!cat merge_list.txt

#Пробуем объединить все файлы в один набор
!plink/plink --threads 6 --bfile ALL.chr1 --merge-list merge_list.txt --make-bed --out ALL.merged

"""##**Найдены трёхаллельные варианты**
Нужно их удалить.

Трехаллельный * triallelic — определение тетраплоидного (Тетраплоид) организма, у которого к.-л. ген представлен тремя разными аллелями (см.): А1А1А2А3, А1А2А2А3 или А1А2А3А3, при этом один из аллелей (А1, А2 или А3) имеется в двух гомологичных хромосомах.
"""

#Удаляем трёхаллельные варианты
for chr_num in range(1, 23):
    !plink/plink --bfile ALL.chr{str(chr_num)} --exclude ALL.merged-merge.missnp --make-bed --out ALL.chr{str(chr_num)}

"""<img src="https://monosnap.com/image/Xxc6Zxih12p4UmhhnSEFnT5WA50lyp" width="300" height="200"/>"""

#Заново сливаем все файлы в один набор
!plink/plink --threads 6 --memory 24576 --bfile ALL.chr1 --merge-list merge_list.txt --make-bed --out ALL.merged

"""# **5. Изменить fam файл, чтобы в нем была информация о популяциях. Соответствие образец-популяция можно найти на сайт 1000 геномов.**
<img src="https://monosnap.com/image/Moe4fRP37bzJ0VNOeiczENXuvUnjQN" width="400" height="200"/>
"""

#Скачиваем таблицу, где можно найти соответствие образец-популяция.
!wget 'http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/working/20130606_sample_info/20130606_sample_info.xlsx'

#Читаем лист "Sample info"
df = pd.read_excel('20130606_sample_info.xlsx', sheet_name = 'Sample Info', header = 0, usecols = ['Sample', 'Population', 'Population Description'])

#Создаем словарь с числовой кодировкой популяций
#Европейские кодируем числом больше > 100
pop_cod = {}
for cnt, samp in enumerate(df.Population.unique()):
    if samp in ['GBR', 'FIN', 'IBS', 'TSI']:
        pop_cod[samp] = cnt + 3 + 100
    else:
        pop_cod[samp] = cnt + 3

#Кодирую популяцию каждого образца в число по ID
df['Popnum'] = 0
ind_cod = {}
for cnt in range(len(df)):
    df.iloc[cnt, 3] = pop_cod[df.Population[cnt]]
    ind_cod[df.iloc[cnt,0]] = pop_cod[df.Population[cnt]]

#Таблица соответствий
for pop, desc in zip(df['Population'].unique(), df['Population Description'].unique()):
    print (f'{pop} - {desc} - {pop_cod[pop]}')

#Считываем fam - файл
df_fam = pd.read_csv('ALL.merged.fam', sep = ' ', header = None, index_col = False)

#Меняем обозначение популяции на число из словаря
for cnt in range(len(df_fam)):
    df_fam.iloc[cnt, 5] = ind_cod[df_fam.iloc[cnt,0]]

#Перезаписываем fam - файл
df_fam.to_csv('ALL.merged.fam', sep = ' ', header = False, index = False, )

"""# **6. Сделать новый набор данных, чтобы в нем были только европейские популяции.**

**Создаём список европейских популяций**
"""

df_keep = df_fam.loc[df_fam[5] > 100][1]

print ('Количество европейских образцов = ', len(df_keep))

df_keep.to_csv('keep.fam', sep = ' ', header = False, index = False, )

"""## **Делаем набор из европейских популяций**"""

!plink/plink --bfile ALL.merged --keep-fam keep.fam --make-bed --out ALL.merged.eu

"""7. http://software.genetics.ucla.edu/admixture/ - с помощью этой программы сделать анализ по 5 компонентам (базовый запуск можно найти в документации)."""

!wget http://software.genetics.ucla.edu/admixture/binaries/admixture_linux-1.3.0.tar.gz

!tar -xvf admixture_linux-1.3.0.tar.gz

"""## **Имеем 404 образца и 78076746 SNPs, каждый SNPs занимает 4 байта в оперативной памяти, следовательно расчетный инстанс:**</font>

404 х 78076746 х 4 / (1024 ^ 3) = 117.5 ГБ оперативной памяти под размещение и ещё ~ 50% от это под инференс, итого ~ 175 ГБ.
Я взял 160 Гб и запуск не упал.
Запускаем 8 итераций на 16 ядрах.

**Время затраченное на расчет = 66016.1 sec = 18 часов 20 минут**

<img src="https://monosnap.com/image/TAFyh4lSalyHAzZ9xQiBwctZP7OP5M" width="400" height="300"/>

20 ядер + 160ГБ оперативки.
"""

!adm/admixture -C 8 ALL.merged.eu.bed 5 -j16

"""# **Q файл по итогам 7 шага**"""

!head ALL.merged.eu.5.Q

"""## Графики разложения отельных индивидов на компоненты
если стартуете с этого пункта минуя admixture, клонируйте git(следующая ячейка)
"""

import sys
!test -d Test || git clone https://github.com/cappelchi/Test.git
if not 'Test' in sys.path:
  sys.path += ['Test']
!ls Test

df_Q = pd.read_csv('Test/ALL.merged.eu.22chr.4pop.5.Q', names = ['comp1', 'comp2', 'comp3', 'comp4', 'comp5'], sep = ' ', header = None, index_col = False)
df_efam = df_fam = pd.read_csv('Test/ALL.merged.eu.22chr.4pop.fam',names = ['ID1', 'ID2', 'clmn3', 'clmn4', 'pop'], sep = ' ', header = None, index_col = False)
df_Q.set_index(df_efam['ID1'])

cod_pop = dict(zip(pop_cod.values(), pop_cod.keys()))

df_efam['pname'] = ''
#df_efam['pcolor'] = ''
for cnt in range(len(df_efam)):
    df_efam.loc[cnt, 'pop'] = ind_cod[df_efam.ID1[cnt]]
    df_efam.loc[cnt, 'pname'] = cod_pop[df_efam['pop'][cnt]]

df_efam

!pip install plotly.express



import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.io as pio
pio.renderers.default = "png"
def enable_plotly_in_cell():
  import IPython
  from plotly.offline import init_notebook_mode
  display(IPython.core.display.HTML('''
        <script src="/static/components/requirejs/require.js"></script>
  '''))
  init_notebook_mode(connected=False)

import plotly.io as pio
from IPython.display import SVG, display

sub_names = []
for cnt in range(len(df_efam)):
    sub_names.append(f'{df_efam.iloc[cnt,0]} - {df_efam.iloc[cnt, 5]}')

enable_plotly_in_cell()
#График первых 40 образцов
def plot_components(start = 0, end = 32, cols = 4):
    rows = (end - start - 1) // cols + 1
    fig = make_subplots(rows = rows, 
                        cols = cols,
                        subplot_titles = sub_names[start:end],                    
                        #vertical_spacing = 0.08,
                        #horizontal_spacing = None
                    )
    for i in range(len(df_efam[start:end])):
        fig.append_trace(go.Bar(
            #x = ['comp1', 'comp2', 'comp3', 'comp4', 'comp5'],
            y = df_Q.iloc[i, :],
            hovertext = df_Q.iloc[i, :]
        ), row = i // cols + 1, col = i % cols + 1)

    #fig.update_layout(height = 300 + len(df_Q)//4 + 1, width=1000, title_text="Stacked subplots")
    fig.update_layout(height = 100 * 10, 
                    width=200 * cols, 
                    title_text=f'Графики разложения отельных индивидов на компоненты c {start} по {end - 1}',
                    showlegend = False
                    )
    fig.show()
plot_components()

"""<img src="https://monosnap.com/image/fb97Kn1cMWH6UYhrHjRYkPVnOEF098"/>

# **Дополнительно:**
посмотрим, как легли популяции на компоненты
"""

import numpy as np

pop_color = {'GBR':'rgb(255, 0, 0)', 'FIN':'rgb(0, 255, 0)', 'IBS':'rgb(0, 0, 255)', 'TSI':'rgb(255, 255, 0)'}

enable_plotly_in_cell()
data =[]
for indv in range(len(df_Q)):
    data.append(go.Scattergl(
        x = [1, 2, 3, 4, 5],
        y = df_Q.iloc[indv,:],
        mode = 'lines + markers',
        hovertext = df_efam.pname[indv],
        marker = dict(        
            color=pop_color[df_efam.pname[indv]],   
            line = dict(width = 1),
            size = 10
        )            

    ))
layout = go.Layout(dict(title = 'Анализ компонент',
                width = 800,
                yaxis = dict(                    
                    title = 'Значение компоненты',                  
                    zeroline = False
                    ),
                xaxis = dict(
                    title = 'GBR - красный, FIN - зелёный, IBS - синий, TSI - желтый',
                    zeroline = False
                    ),
                showlegend = False
                ))
#data = [trace0]
fig = dict(data = data, layout = layout)
py.iplot(fig, show_link = False)

"""<img src="https://monosnap.com/image/q1sciAgP1nKhDfI8qEtTjGgM57n1U9"/>

Видно, что очень хорошо легли финские образцы, что и логично, учитывая уникальность её популяции.

Методом k-средних(K-means), который используется для кластеризации данных на основе алгоритма разбиения, кластеризуем компоненты. При кластеризации методом k-средних количество кластеров чаще всего оценивают с помощью «метода локтя» (отношения внутрикластерного расстояния к межкластерному расстоянию).
"""

from sklearn.cluster import KMeans

cost = []
clust =[]
for k in range(2, 17):
    kmeans = KMeans(
        n_clusters = k, 
        #init='random',
        n_init=10, 
        max_iter=300, 
        tol=1e-04, 
        random_state=0
    ).fit(df_Q)
    inert = kmeans.inertia_
    print ("k:",k, " cost:", inert)
    cost.append(inert)
    clust.append(k)

enable_plotly_in_cell()
trace0 = go.Scatter(
    x = clust,
    y = cost ,
    name = 'K means clusters count'
    )
layout = go.Layout(dict(title = 'K - MEANS CLUSTER COUNT OPTIMIZING',
              yaxis = dict(
                  width = 800,
                  height = 600,
                  title = 'Inertia',                  
                  zeroline = False
                  ),
              xaxis = dict(
                  title = 'Cluster quantity',
                  zeroline = False
                  )
             ))
data = [trace0]
fig = dict(data = data, layout = layout)
py.iplot(fig, show_link = False)

"""<img src="https://monosnap.com/image/PjH2MSufiRs8aPf6Y6Sfr41F1yxAUn"/>

Оптимальное количество кластеров по методу локтя = 5.
"""

kmeans = KMeans(
        n_clusters = 5, 
        #init='random',
        n_init=10, 
        max_iter=300, 
        tol=1e-04, 
        random_state=0
    ).fit(df_Q)

df_efam['cluster'] = kmeans.labels_

"""3-й кластер полностью финский остальные смешанные"""

df_efam.groupby(['cluster', 'pname'])['ID1'].count()

"""Попробуем другой метод выбора количество кластеров с максимизацией одной популяции в кластере."""

cost = []
clust =[]
kf = []
for k in range(2, 40):
    kmeans = KMeans(
        n_clusters = k, 
        #init='random',
        n_init=10, 
        max_iter=300, 
        tol=1e-04, 
        random_state=0
    ).fit(df_Q)
    inert = kmeans.inertia_
    df_efam['cluster'] = kmeans.labels_
    clust_k = sum([df_efam.groupby([df_efam.cluster == x, 'pname'])['ID1'].
                   count().reset_index().query('cluster == True').max().ID1 
                    for x in range(kmeans.labels_.max() + 1)]) - 2 * k
    print ("k:",k, " cost:", inert, " clust_k: ", clust_k)
    cost.append(inert)
    clust.append(k)
    kf.append(clust_k)

enable_plotly_in_cell()
trace0 = go.Scatter(
    x = clust,
    y = kf ,
    name = 'K means clusters count'
    )
layout = go.Layout(dict(title = 'K - MEANS CLUSTER COUNT OPTIMIZING',
              yaxis = dict(
                  title = 'Inertia',                  
                  zeroline = False
                  ),
              xaxis = dict(
                  title = 'Cluster qulity',
                  zeroline = False
                  )
             ))
data = [trace0]
fig = dict(data = data, layout = layout)
py.iplot(fig, show_link = False)

"""<img src="https://monosnap.com/image/SZt28mpDs12o5nACs7CPAlxRony48j"/>

Здесь пик на 12-ти кластерах.
"""

kmeans = KMeans(
        n_clusters = 12, 
        #init='random',
        n_init=10, 
        max_iter=300, 
        tol=1e-04, 
        random_state=0
    ).fit(df_Q)

df_efam['cluster'] = kmeans.labels_

df_efam.groupby(['cluster', 'pname'])['ID1'].count()

"""1 и 11 кластеры полностью финские = 99 образцов

2 и 8 - полностью GBR = 30 (из 91)

0 и 6 и 7 - TSI = 46 (из 107)

IBS - особенно хорошо нигде не локализовался

Линейно разделить все популяции на основе 5 компонентов получается плохо. Нужно пробовать нелинейные подходы типа SOM, либо увеличить количество компонентов, возможно европейские популяции сильно смешаны (кроме финов), поэтому их сложно разделить.
"""